\newpage
\vspace{-.25in}
\section{COMPUTATIONAL READINESS} % typically about 5 pages
\vspace{-.2in}
%Why is nekRS ready for the task? What simulations have we already done?

All proposed simulations will be performed on GPUs using a GPU-oriented version
of Nek5000. Nek5000 is a highly scalable open-source code for thermal-fluid 
simulation and a Gordon Bell Prize winner \cite{tufo99} with a long development
history on leading-edge parallel platforms. This new GPU-oriented version of Nek5000,
{\it NekRS}, is written in C++ and is being developed as part of DOE's Exascale Computing
Project in the Center for Efficient Exascale Discretizations (CEED). {\bf mention ECP?}

NekRS is based on high-performance kernels from the libParanumal library
out of the Warburton group at Viginia Tech and is written in the open
concurrent computed abstraction (OCCA) for platform portability.  OCCA
is a decorated C extension that supports multiple back-ends, including CUDA,
HIP, and OpenCL. For the NVIDIA platforms, OCCA generates code that matches
the roofline performance of hand-written CUDA. 

We propose to use Summit and Frontier for all proposed simulations. Past experience shows that
NekRS typically
runs 12--14 times faster than Nek5000 on the Summit nodes because of the
relative performance of the 6 V100s compared to the 42 CPU cores (he
principal kernels are sustaining 1-2 Tflops (fp64) on the Nvidia V100s
\cite{fischer20a,warburton2019}.) As part of the CEED mandate, OCCA supports
backends for Nvidia and AMD architectures, so we expect to have state of the
art performance for Frontier as we already do for Summit.

\vspace{-.25in}
\subsection{Use of Resources Requested}
\vspace{-.2in}

The tasks in the project will include numerous cases reflective of different
non-dimensional numbers. The cases are summarized in Table~\ref{tab:cases}.

\begin{table}
\centering
\caption{Summary of cases in the project}
\begin{tabular}{rrrrrr}
\hline
\hline
Task & Max Size ($E$) &  $n$, \# of cases  & $t$, \# of steps & node-hours & Total Storage (TB)\\
\hline
\hline
1 & 5 million       & 12   & 500,000   &    130,000 & 51\\
2 & 6 million       & 4    & 3,000,000 &    210,000 & 40.8\\
3 & 12 million      & 8    & 3,000,000 &    840,000 & 163.2\\
4 & ~90 million  & 6    & 500,000   &  1,170,000 & 459\\
\hline
\hline
\end{tabular}
\label{tab:cases}
\end{table}

The estimates of the max number of degree of freedom are based  on past
experience and the estimated Reynolds numbers. Similar estimates on the number
of time steps to collect statistics are provided. For Tasks 2 and 3,
considerably longer integration times are expected due to either larger scale
separation or the need to collect statistics on a significant number of thermal
fluctuations. All production runs will be performed at polynomial order $N=7$.

As discussed later and presented in Table~\ref{wscaling2} previous weak
calculations have shown that for a 17x17 fuel assembly we can expect a time
step on Summit to cost conservatively 0.7 s for 45,000 elements per node at $N=7$. We
used this data to estimate the number of node-hours
($n\frac{E}{45000}\frac{0.7t}{3600}$).

The total request over 3 years is 2,350,000 node-hours in total. With a 10\% increase to allow for testing and debugging we arrive to 2,584,000 node-hours. Given the milestone table we request:
\begin{itemize}
    \item Year 1, half of the computational request for Task 1 plus one third of the request for Task 2-4, for a total of 885,000 node-hours;
    \item Year 2, half of the computational request for Task 1 plus one third of the request for Task 2-4, for a total of 885,000 node-hours;
    \item Year 3, one third of the request for Task 2-4, for a total of 814,000 node-hours.
\end{itemize}
All simulations are to be conducted on Summit. Almost all our production runs will be performed in the capability queue of Summit and thus can be met only at the OLCF.

In terms of storage we estimate the need to store at least 50 restart files for each case (200 for Task 2 and 3 due to the longer transients) including turbulence budgets. A typical Nek5000 restart files requires 17 GB per 1,000,000 elements. The total estimates for filesystem storage are listed in Table~\ref{tab:cases}. To total for all tasks over 3 years is 714 TB.

Offline storage is estimates at 4 times the total filesystem storage estimate to preserve the  data of previous runs (2.85 PB).

We note that the proposed work is modular and if a reduction in scope is necessary given limited resources,  it can be accomodated by reducing the number of cases for each task.

%Describe your proposed production simulations and state how the runs are tied to each of your project's goals and milestones (Section 4, "Milestone Table"). For the simulations you plan to carry out during production runs, provide a
%\begin{enumerate}[noitemsep,topsep=0pt]
%\item Description of what jobs are going to be run and how they relate to the research/development objectives and milestones given above;
%\item Description of processor/core use for large runs (e.g., 10,000-hour run with 100 cores, or ten 10-hour runs with 10,000 cores, for a 1,000,000-hour allocation).  For the XK7, indicate which of these production simulations employ the GPUs.
%\item Clear, detailed explanation as to how you calculated the requested number of processor hours; and
%\item Summary of your anticipated annual burn rate (e.g., linear or with periods of peak usage).\\
%\end{enumerate}

%\vspace{.1in}
%Also describe the data requirements of your production simulations.  If at any point during your project the sum of your data storage needs in the scratch filesystems exceed 1 petabyte, specific justification is required. For your production simulations, provide a:

%\begin{enumerate}[noitemsep,topsep=0pt]
%\setcounter{enumi}{4}
%\item Estimate and breakdown of the anticipated cumulative size of stored data, in scratch and long-term archival storage, at the end of the requested award.
%\item Description of the effective lifetime of your stored data.  If the lifetime varies, show the breakdown by the total size used.  Explain the reason for the lifetime.
%\item Description of the data, including the expected size of the data, which will be transferred into or out of the center.  Describe what tools for transferring the data from external sources will be used.
%\item Description of the tools for data storage, compression (reduction), and analysis that you currently use. Describe whether the tools and/or applications needed are ready or whether there new capabilities or features that must be developed.
%\item If you are intending to make any fraction of the data generated public, specify:
%\begin{enumerate}[noitemsep,topsep=0pt]
%\item How much data and the scientific purpose
%\item What tool will be used to share the data
%\item From where will the data be shared\\
%\vspace{.1in}
%NOTE: The LCF data management policies can be found at
%
%OLCF:  {\href{https://www.olcf.ornl.gov/computing-resources/data-management/data-management-user-guide/}{https://www.olcf.ornl.gov/computing-resources/data-management/data-management-user-guide/}}
%
%ALCF:  {\href{http://www.alcf.anl.gov/user-guides/data-policy}{http://www.alcf.anl.gov/user-guides/data-policy}}
%\end{enumerate}
%\end{enumerate}

\vspace{-.25in}
\subsection{Computational Approach}
\vspace{-.2in}

In this section we discuss the computational approach of the proposed work based on Nek5000 and NekRS.
%
Nek5000 (1999 Gordon Bell and 2016 R\&D 100 award winning code) is an
open-source simulation-software package that delivers highly accurate solutions
for a wide range of scientific applications including fluid flow, thermal
convection, combustion, and magnetohydrodynamics. It features state-of-the-art,
scalable, high-order algorithms that are fast and efficient on platforms
ranging from laptops to the DOE leadership computing facilities.
({\footnotesize\url{http://nek5000.mcs.anl.gov}})
   Significant applications of Nek5000 include DOE scientific
computing mission areas (reactor, combustion, ocean, wind, etc.) with over 400
users in academia, laboratories, and industry. Its central role in other DOE
projects includes  ECP (CEED, ExaSMR, Urban, Combustion), PSAAP-II, NEUP, NEAMS, NE
High-Impact Project (HIP) and  INL-ANL Center for Thermal Fluid Applications in
Nuclear Energy.
   Active users of Nek5000 are industrial firms AREVA, Westinghouse,
TerraPower, NRG (Energy Research Centre of the Netherlands), and BOSCH, and
universities ETH Zurich, KTH Royal Institute of Technology, ENSAM (Paris),
Texas A\&M, University of Miami, University of Florida, University of Maryland,
Baltimore County, and the University of Illinois Urbana Champaign.

NekRS is a new C++ variant of Nek5000 being developed at Argonne as part of the
ECP Center for Efficient Exascale Discretizations and is the version primarily
used in this work.  It is based
on OCCA and libParanumal, both out of the group of Tim Warburton at
Virginia Tech.  OCCA supports both CUDA and HIP backends (for Nvidia and AMD)
and highly-tuned OCCA kernels that realize roofline-limited performance
(1--2 Tflops, fp64, on the V100) are available in libParanumal \cite{fischer20a}.

\input tex/performance

\vspace{-.25in}
\subsection{Developmental Work}
\vspace{-.2in}

% For the computational approach described above, describe what, if any, development work has been carried out to-date, especially on the architecture of the requested resource. Describe what development work will be executed during the proposed INCITE campaign and when it will be executed. Provide an estimate of the computational resources required for this work. If applicable, identify the milestones and production activities in Section 2.3.i that are dependent on the developmental work and provide a plan for validating this developmental work.

\vspace{-.15in}
\section{REFERENCES}
\vspace{-.15in}

%References are optional and may be structured in accordance with any style. They {\bf \em {do}} count toward the 15-page limit.


\renewcommand{\section}[2]{}%	No 'References' title
%\renewcommand{\chapter}[2]{}% for other classes


\bibliographystyle{ieeetr}
\bibliography{references,ref_nt,emmd}

\end{document}

