\newpage
\vspace{-.25in}
\section{COMPUTATIONAL READINESS} % typically about 5 pages
\vspace{-.2in}


The proposed simulations will be performed using a GPU-oriented version of
Nek5000, which is a highly scalable open-source code for fluid-thermal
simulation and a Gordon Bell and RD 100 Prize winner \cite{tufo99}.  This new
code, NekRS, is based on high-performance kernels from the libParanumal library
out of the Warburton group at Viginia Tech \cite{warb1,warb2} and is written in
the open concurrent computed abstraction (OCCA) for platform portability
\cite{occa}.  NekRS, libParanumal, and OCCA are being developed as part of
DOE's Exascale Computing Project in the Center for Efficient Exascale
Discretizations (CEED). 

\noindent{\em State-of-the-Art.}
The numerical discretization of the Navier-Stokes and thermal-transport equations
in Nek5000/RS is based on the spectral element method (SEM) \cite{pat84}, which
is a high-order weighted residual formulation similar to the finite-element method.   
The SEM derives its computational efficiency from matrix-free operator
evaluations that allow the matrix-vector products required for iterative
solvers to be expressed as fast tensor contractions.  These contractions
require only $O(n)$ storage and $O(nN)$ work for a discretization involving
$n=EN^3$ gridpoints, where $E$ is the number of spectral elements and $N$ is
the discretization order (typically, $N$=6--10) \cite{dfm02}.  This low
complexity contrasts sharply with that of the $p$-type FEM, which has work and
storage complexity of $O(nN^3)$ that effectively precludes using $N>3$.  

Because of their ability to accurately transport small-scale flow features over
large domains (for a given resolution, $n$) high-order discretizations have
been the mainstay of DNS and LES turbulence simulations since the early 70s 
\cite{kreiss72,sao72}.   The SEM allows one to employ high-order methods in 
complex domains relevant to reactor analysis  \cite{pat84,sao80}.   
The central idea is to map
quantities from a reference element, $\Oh:=[-1,1]^3$ to curvilinear brick
elements, $\Omega^e$, $e=1,\dots,E$.  On each element, $e$, the velocity
(temperature, geometry, etc.) is expressed as a tensor-product polynomial
of the form
\begin{eqnarray}
  u^e(\xi,\eta,\zeta) = \sum_{ijk} u_{ijk}^e l_i(\xi) \, l_j(\eta) \, l_k(\zeta),
\end{eqnarray}
where $l_i(\xi)$ is a Lagrange polynomial based on the Gauss-Lobatto-Legrendre
(GLL) quadrature points, $\xi_j$, such that $l_i(\xi_j)=\delta_{ij}$, with the
$i$-$j$-$k$ in the index range $\in [0:N]^3$.  Operations that map a set of input
function values to the corresponding outputs are simple tensor contractions.
%% that constitute $\approx 90$\% of the flops in Nek5000.
For example, the derivative $w := u_{\xi}$ at each quadrature point would be
expressed as $w^e_{ijk} = \sum_p \, \Dh_{ip} u_{pjk}^e$, with the derivative
matrix, $\Dh_{ip} := \left. \dd{l_p}{\xi} \right|_{\xi_i}$.   
In typical production runs, $N$=7, which means that each element comprises 512
values, $u_{ijk}^e$, that are addressed lexicographically with no indirect
addressing.\footnote{Indirect addressing---with communication---is used,
however, to {\em assemble} the residuals at element interfaces \cite{dfm02}.},
The derivative can be evaluated with $\approx n$ memory references and
$\approx 2nN$ operations, which yields a favorable $O(N)$ flops-to-byte ratio 
(computational intensity).
Differentiation in physical space on $\Omega^e$ is performed by the chain rule, 
(e.g., $u_x = 
u_{\xi}   \mbox{\footnotesize $\xi_x$}    \,+\, 
u_{\eta}  \mbox{\footnotesize $\eta_x$}   \,+\, 
u_{\zeta} \mbox{\footnotesize $\zeta_x$}  $), 
which is applied pointwise at the GLL nodes in $\approx 5n$ operations.  
The principal advantage of the SEM is that the error in the polynomial
interpolants decreases exponentially fast with $N$, {\em error}$\sim e^{-\sigma
N}$, $\sigma > 0$, when the solution is smooth (i.e., resolved).
This is in contrast to algebraic convergence, such as $O(h^2)$, attained
for classical second-order methods.




Recent studies
have demonstrated that the SEM formulation in Nek5000 significantly outperforms
the finite-volume code OpenFOAM for this class of flow problems.  Capuano {\em
et al.} \cite{capuano19} report that OpenFOAM requires 7 times the number of
gridpoints as Nek5000 to realize the same accuracy.  They also report, however,
that Nek5000 requires a smaller timestep for numerical stability, but they did
not use the characteristics-based timestepper in Nek5000, which allows one to
use an order-of-magnitude larger stepsize without compromising stability
\cite{patel18}.  Rezaeiravesh {\em et al.} \cite{schlatter21} illustrate that,
for the same number of gridpoints, Nek5000 outperforms OpenFOAM by an a factor
of 12 to 15 at the strong-scale limit of $n/P \approx 6000$ ($P=960$ cores) on
the Cray-XC40, Beskow.  At this limit, Nek5000 continues to exhibit linear
scaling, which means that, for the same computational resources (node hours and
joules), the solution time is reduced by an order of magnitude.  With a reduced
number of gridpoints ($n$) and time-per-gridpoint (seconds per
degree-of-freedom) we can conclude that Nek5000/RS is well suited for the 
proposed high-fidelity simulations.

Scalable multilevel iterative solvers are critical to the overall performance
of Nek5000/RS.   We currently support a variety of preconditioners for the
pressure Poisson problem, which is the stiffest substep in time advancement of
the incompressible Navier-Stokes equations.  For Nek5000 and NekRS, we support
Chebyshev-accelerated (in NekRS only) $p$-multigrid (PMG) with overlapping
Schwarz based smoothers \cite{lottes05,nekrs} and algebraic multigrid (AMG)
applied to a surrogate FEM discretization of the Poisson problem
\cite{pedro19,sao80}.  This latter approach is somewhat expensive, but
invariably yields low iteration counts even when the mesh is ill-conditioned
\cite{fischer97}.  Our preconditioners have demonstrated strong scalability to
beyond a million MPI ranks (60\% efficiency at $\approx 2000$ points/rank) with
fast AMG-based coarse-grid solvers \cite{fischer15}.


\noindent{\em Parallel Performance.}
NekRS has been developed in close collaboration with the libParanumal project
\cite{warburton2019,warburton2019b,ChalmersKarakusAustinSwirydowiczWarburton2020,streamParanumal2020},
which provides high-performance kernels for high-order methods on GPUs.  The
kernels are written in OCCA to abstract between different parallel languages
such as OpenCL, CUDA, and HIP. OCCA allows developers to implement parallel
kernel code in a slightly decorated C++ language, OKL.  At runtime, the user
can specify which parallel programming model to target, after which OCCA
translates the OKL source code into the desired target language and
Just-In-Time (JIT) compiles kernels for the user's target hardware
architecture.  In the OKL language, parallel loops and variables in special
memory spaces are described with simple attributes. For example, iterations of
nested parallel {\em for} loops in the kernel are annotated with
\texttt{@outer} and \texttt{@inner} to describe how they are to be mapped to a
grid of work-item and work-groups in OpenCL or threads and thread-blocks in
CUDA and HIP. All iterations that are annotated with \texttt{@outer} or
\texttt{@inner} are assumed to be free of loop carried dependencies. 
The OCCA-based libParanumal kernels
typically saturate the roofline on the NVIDIA GPUs, as illustrated
in Fig. \ref{fig:roof}, where we see that the local Poisson kernel
(without parallel communication) realizes 1.5 TFLOPS FP64 on a single V100
of Summit \cite{ceed_bp_paper_2020}.

Through extensive performance tuning including covering communication with
computation, use of mixed precision in preconditioners to lower
injection-bandwidth pressure, improved preconditioners, and run-time optimized
communication strategies that select between GPU-direct, GPU-packed+host-based,
or pure host-based communication, we have made signifcant strides in delivering
performance for NekRS \cite{nekrs}.  On Summit, it is significantly faster at
its strong-scale limit ($n/P \approx $2.5M, where $P$ is the number of V100s),
than Nek5000 is on Mira at its strong-scale limit ($n/P \approx 4000$, where $P$
is the number of ranks in -c32 mode).\footnote{In both cases, we identify the
strong-scale limit as the value of $n/P$ where the code realizes $\approx$80\%
efficiency, which is commensurate with production runs and therefore our
target performance design point.}  Production runs at the strong-scale
limit realize about 0.5--1.0 seconds per step for Nek5000 on Mira, while NekRS
typically requires about 0.1--0.4 seconds per step on Summit,  as illustrated
in Table \ref{tab:spacer}.  Moreover, production Nek5000 runs on all or half of
Mira were limited to about 15M elements ($n \approx 5$B), whereas 100M-element
runs with $n \approx 50$--75B are readily accessible on Summit with NekRS
operating at its strong-scale (i.e., {\em fast}) limit.

\newpage
In Tables \ref{tab:weak} and \ref{tab:strong} we present weak- and strong-scaling
results for NekRS for a 17$\times$17 rod-bundle configuration.


\input tex/performance

\noindent{\em Project Workflow and I/O.}
The scale of the proposed problems on Summit and Frontier is signifcantly
larger than that of prior production runs on Mira.  For example, full-core
simulations of the pebble bed reactor  ($n$=50B) on all of Summit generate
single snapshot files of 1.4 TB, each, with 32 bit words.  Using MPI I/O, these
files are written in 40 seconds (32 GB/sec) for simulations that take about 0.4
seconds per step, which is tolerable.   Visualization and file movement,
however, are quite painful and we have had to develop specialized subsampling
techniques that collect only the ``visible'' elements to make image rendering
feasible.  We envision that compression techniques based on spectral (Legendre)
data representations will be useful for reducing file sizes; compression ratios
of up to 40:1 have been reported for this class of problems when applied to
Nek5000 \cite{otero18}.  Additionally, we are able to use Nek5000/RS to perform
all of the data analysis, at scale.  Both codes collect statistics (e.g., mean,
RMS, and cross-correlations) for velocity and temperature at runtime and
extensive turbulent-statistics post-processing utilities are available for
Nek5000 that are compatible with NekRS. 

Another concern is construction and use of very large meshes.  The record
for Nek5000 on Mira was 15M elements.  We have recently extended Nek5000/RS
to support over 1B elements ($n$=216B).   This extension required promoting
several arrays to support int8 indexing during problem-setup.  (These
are arrays that are used to identify face-face and edge-edge connections
between elements---the array sizes scale as $E/P$ and are thus fully
scalable.)   Fortunately, for mesh construction, we can use many of the 
available FEM tools because the number of elements required for the SEM
is two orders of magnitude smaller than that required for low-order FEM 
simulations at the same resolution.


\vspace{-.25in}
\subsection{Developmental Work}
\vspace{-.2in}

% For the computational approach described above, describe what, if any,
% development work has been carried out to-date, especially on the architecture
% of the requested resource. Describe what development work will be executed
% during the proposed INCITE campaign and when it will be executed. Provide an
% estimate of the computational resources required for this work. If
% applicable, identify the milestones and production activities in Section
% 2.3.i that are dependent on the developmental work and provide a plan for
% validating this developmental work.

\vspace{-.15in}
\section{REFERENCES}
\vspace{-.15in}

%References are optional and may be structured in accordance with any style. They {\bf \em {do}} count toward the 15-page limit.


\renewcommand{\section}[2]{}%	No 'References' title
%\renewcommand{\chapter}[2]{}% for other classes


\bibliographystyle{ieeetr}
\bibliography{references,ref_nt,emmd}

\end{document}

